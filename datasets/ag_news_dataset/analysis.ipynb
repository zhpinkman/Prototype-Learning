{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "631\n",
      "461\n"
     ]
    }
   ],
   "source": [
    "for attack in [\"deepwordbug\", \"textfooler\", \"textbugger\"]:\n",
    "    files = [file for file in os.listdir() if file.startswith(f\"adv_{attack}\")]\n",
    "    dfs = [pd.read_csv(file) for file in files]\n",
    "    all_original_texts = [df[\"original_text\"].values for df in dfs]\n",
    "    all_perturbed_texts = [df[\"perturbed_text\"].values for df in dfs]\n",
    "    all_labels = [df[\"label\"].values for df in dfs]\n",
    "\n",
    "    x1 = all_original_texts[0]\n",
    "    x1_clean = [i.replace(\"[\", \"\").replace(\"]\", \"\") for i in x1]\n",
    "    x1_clean_to_x1 = {i: j for i, j in zip(x1_clean, x1)}\n",
    "    y1 = [x.replace(\"[\", \"\").replace(\"]\", \"\").split() for x in all_perturbed_texts[0]]\n",
    "    x1_clean_to_y1 = {i: j for i, j in zip(x1_clean, y1)}\n",
    "    labels1 = all_labels[0]\n",
    "    x1_clean_to_labels1 = {i: j for i, j in zip(x1_clean, labels1)}\n",
    "\n",
    "    x2 = all_original_texts[1]\n",
    "    x2_clean = [i.replace(\"[\", \"\").replace(\"]\", \"\") for i in x2]\n",
    "    x2_clean_to_x2 = {i: j for i, j in zip(x2_clean, x2)}\n",
    "    y2 = [x.replace(\"[\", \"\").replace(\"]\", \"\").split() for x in all_perturbed_texts[1]]\n",
    "    x2_clean_to_y2 = {i: j for i, j in zip(x2_clean, y2)}\n",
    "\n",
    "    x3 = all_original_texts[2]\n",
    "    x3_clean = [i.replace(\"[\", \"\").replace(\"]\", \"\") for i in x3]\n",
    "    x3_clean_to_x3 = {i: j for i, j in zip(x3_clean, x3)}\n",
    "    y3 = [x.replace(\"[\", \"\").replace(\"]\", \"\").split() for x in all_perturbed_texts[2]]\n",
    "    x3_clean_to_y3 = {i: j for i, j in zip(x3_clean, y3)}\n",
    "\n",
    "    x_clean_commons = set.intersection(set(x1_clean), set(x2_clean), set(x3_clean))\n",
    "    print(len(x_clean_commons))\n",
    "\n",
    "    results_original_texts = []\n",
    "    results_perturbed_texts = []\n",
    "    results_labels = []\n",
    "\n",
    "    for x_clean in x_clean_commons:\n",
    "        x_clean_splited = x_clean.split()\n",
    "        a = x1_clean_to_x1[x_clean]\n",
    "        b = x2_clean_to_x2[x_clean]\n",
    "        c = x3_clean_to_x3[x_clean]\n",
    "\n",
    "        a_ = x1_clean_to_y1[x_clean]\n",
    "        b_ = x2_clean_to_y2[x_clean]\n",
    "        c_ = x3_clean_to_y3[x_clean]\n",
    "\n",
    "        a_parts_to_replace = [\n",
    "            i\n",
    "            for i, text in enumerate(a.split())\n",
    "            if text.startswith(\"[[\") and text.endswith(\"]]\")\n",
    "        ]\n",
    "        b_parts_to_replace = [\n",
    "            i\n",
    "            for i, text in enumerate(b.split())\n",
    "            if text.startswith(\"[[\") and text.endswith(\"]]\")\n",
    "        ]\n",
    "        c_parts_to_replace = [\n",
    "            i\n",
    "            for i, text in enumerate(c.split())\n",
    "            if text.startswith(\"[[\") and text.endswith(\"]]\")\n",
    "        ]\n",
    "\n",
    "        resulted_text_parts = []\n",
    "\n",
    "        for i in range(len(x_clean.split())):\n",
    "            if i in a_parts_to_replace:\n",
    "                # print(\"a\", a_[i])\n",
    "                resulted_text_parts.append(a_[i])\n",
    "            elif i in b_parts_to_replace:\n",
    "                # print(\"b\", b_[i])\n",
    "                resulted_text_parts.append(b_[i])\n",
    "            elif i in c_parts_to_replace:\n",
    "                # print(\"c\", c_[i])\n",
    "                resulted_text_parts.append(c_[i])\n",
    "            else:\n",
    "                # print(\"-\", x_clean_splited[i])\n",
    "                resulted_text_parts.append(x_clean_splited[i])\n",
    "        resulted_text = \" \".join(resulted_text_parts)\n",
    "        results_original_texts.append(x_clean)\n",
    "        results_perturbed_texts.append(resulted_text)\n",
    "        results_labels.append(x1_clean_to_labels1[x_clean])\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"original_text\": results_original_texts,\n",
    "            \"perturbed_text\": results_perturbed_texts,\n",
    "            \"label\": results_labels,\n",
    "        }\n",
    "    ).to_csv(f\"adv_{attack}_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
