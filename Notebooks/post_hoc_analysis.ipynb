{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/cluster/raid/home/himanshu.rawlani/propaganda_detection/prototex_custom_latest/Notebooks', '/cluster/raid/home/himanshu.rawlani/miniconda3/envs/prototex/lib/python39.zip', '/cluster/raid/home/himanshu.rawlani/miniconda3/envs/prototex/lib/python3.9', '/cluster/raid/home/himanshu.rawlani/miniconda3/envs/prototex/lib/python3.9/lib-dynload', '', '/cluster/raid/home/himanshu.rawlani/miniconda3/envs/prototex/lib/python3.9/site-packages', '../']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "MOD_FOLDER = '../'\n",
    "# setting path to enable import from the parent directory\n",
    "sys.path.append(MOD_FOLDER)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"../data/logical_fallacy/edu_train.csv\")\n",
    "dev_df = pd.read_csv(\"../data/logical_fallacy/edu_dev.csv\")\n",
    "test_df = pd.read_csv(\"../data/logical_fallacy/edu_test.csv\")\n",
    "\n",
    "train_df = train_df[train_df['updated_label'] != 'equivocation']\n",
    "dev_df = dev_df[dev_df['updated_label'] != 'equivocation']\n",
    "test_df = test_df[test_df['updated_label'] != 'equivocation']\n",
    "\n",
    "\n",
    "train_labels = train_df['updated_label'].tolist()\n",
    "test_labels = test_df['updated_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestk_train_data_per_proto = joblib.load(\"bestk_train_data_per_proto.joblib\")\n",
    "best_protos_per_testeg = joblib.load(\"best_protos_per_testeg.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1728,  733, 1284, 1569, 1431],\n",
       "       [ 659, 1216,  212, 1292, 1111],\n",
       "       [ 468,  130, 1578, 1216, 1480],\n",
       "       [1612,  300,  260,  271,   66],\n",
       "       [1557,  941,  352, 1194, 1157],\n",
       "       [  36, 1597,   87,  402,   55],\n",
       "       [1444,   20, 1485, 1179,  301],\n",
       "       [1449, 1363, 1235,   10,  771],\n",
       "       [1155, 1181,  961,  943,  773],\n",
       "       [1237,    8, 1627,  683, 1442]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestk_train_data_per_proto[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 22,  6, 27,  7],\n",
       "        [ 5, 26, 27, 16,  4],\n",
       "        [ 5, 34,  9, 35, 14],\n",
       "        [ 5, 14, 31, 24, 27],\n",
       "        [ 5, 22, 24, 15, 27],\n",
       "        [ 5, 28,  7, 20, 27],\n",
       "        [ 5, 28,  7, 20,  8],\n",
       "        [ 5, 25, 12, 27, 16],\n",
       "        [ 5, 25, 27, 16, 29],\n",
       "        [ 5, 22, 15, 24, 32],\n",
       "        [ 5, 14,  1, 31, 17],\n",
       "        [ 5, 27, 16, 14, 31],\n",
       "        [ 5, 24, 12, 27,  7],\n",
       "        [34,  5,  9,  0, 35],\n",
       "        [ 5, 27, 16, 22, 26],\n",
       "        [ 5, 22, 16, 27, 11],\n",
       "        [34,  9,  0, 35,  5],\n",
       "        [ 5, 23, 27, 16, 34],\n",
       "        [ 5, 25, 12, 30, 24],\n",
       "        [ 5, 24, 18, 32, 22]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_protos_per_testeg[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for test_index, test_sample_prototypes in enumerate(best_protos_per_testeg[0]):\n",
    "    test_sample_label = test_labels[test_index]\n",
    "    respective_prototypes_train_labels = []\n",
    "    for prototype in test_sample_prototypes:\n",
    "        prototype_train_labels = []\n",
    "        train_samples_close_to_prototype = bestk_train_data_per_proto[0][prototype]\n",
    "        for train_sample in train_samples_close_to_prototype:\n",
    "            prototype_train_labels.append(train_labels[train_sample])\n",
    "        respective_prototypes_train_labels.append(prototype_train_labels)\n",
    "    results.append((test_sample_label, respective_prototypes_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_of_model_on_label(results, k = 5):\n",
    "    statistics = []\n",
    "    for test_label, prototypes_train_labels in results:\n",
    "        per_test_statistics = []\n",
    "        for prototype_train_labels in prototypes_train_labels:\n",
    "            per_test_statistics.append(test_label in prototype_train_labels)\n",
    "        statistics.append(per_test_statistics)\n",
    "    statistics = np.array(statistics)\n",
    "    return np.round(np.mean(np.sum(statistics[:,:k], axis = 1) != 0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "accuracies = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [5, 3, 1]:\n",
    "    overall_accuracy = get_accuracy_of_model_on_label(results, k)\n",
    "    accuracies[k].append(overall_accuracy)\n",
    "    for label in set(train_labels):\n",
    "        label_specific_accuracy = get_accuracy_of_model_on_label([result for result in results if result[0] == label], k)\n",
    "        accuracies[k].append(label_specific_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame(\n",
    "    {\n",
    "        'class': [\n",
    "            'overall',\n",
    "            *list(set(train_labels))\n",
    "        ],\n",
    "        'k = 5': accuracies[5],\n",
    "        'k = 3': accuracies[3],\n",
    "        'k = 1': accuracies[1]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>k = 5</th>\n",
       "      <th>k = 3</th>\n",
       "      <th>k = 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overall</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>false causality</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appeal to emotion</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faulty generalization</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fallacy of logic</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fallacy of extension</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>circular reasoning</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fallacy of credibility</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>false dilemma</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ad populum</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fallacy of relevance</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ad hominem</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>intentional</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     class  k = 5  k = 3  k = 1\n",
       "0                  overall  0.861  0.725  0.359\n",
       "1          false causality  1.000  0.722  0.000\n",
       "2        appeal to emotion  0.261  0.043  0.000\n",
       "3    faulty generalization  1.000  1.000  1.000\n",
       "4         fallacy of logic  0.357  0.143  0.000\n",
       "5     fallacy of extension  1.000  0.952  0.000\n",
       "6       circular reasoning  1.000  1.000  1.000\n",
       "7   fallacy of credibility  0.588  0.471  0.000\n",
       "8            false dilemma  1.000  0.917  0.000\n",
       "9               ad populum  1.000  0.533  0.400\n",
       "10    fallacy of relevance  0.667  0.500  0.000\n",
       "11              ad hominem  1.000  0.902  0.000\n",
       "12             intentional  1.000  0.933  0.933"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('prototex': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67a2a78c27fe286f44f928febc9f5314f2b48ea11236d1a7eff6379cd9712b1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
